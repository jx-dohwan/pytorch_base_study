{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 확률적 경사하강법\n",
    "- 전체 데이터를 모두 사용하여 기울기를 구하면 학습하는데 많은 시간이 필요하다.\n",
    "- 이러한 단점을 보완하기 위해서 확률적 경사하강법을 사용할 수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 확률적 경사하강법이란?\n",
    "- 지금까지 우리는 파라미터를 업데이트 하귀 위해 데이터셋의 모든 샘플들을모델에 통과 시킨후 손실값을 계산했다.\n",
    "- 그리고 손실 ㄱ밧을 가중치 파라미터들로 미분하여 파라미터 업데이트를 수행할 수 있었다.\n",
    "- 즉, N개의 샘플이 존재할 때 한번의 파라미터 업데이트를 위해 N개 샘플들이 모두 모델을 통과해야 했다.\n",
    "- 다행히도 GPU의 병렬 연산을 활용하여 N개의 입력을 한 번에 통과시켜 손실 값을 계산할 수 있었다.\n",
    "- 한계 : 이러한 방법은 데이터셋이 큰 경우 여러 문제점이 생길 수 있다.\n",
    "    - GPU 메모리는 한계가 있기 때문에 큰 데이터셋을 한 번에 계산하는 것이 어려울 수 있다.\n",
    "        - 이럴때는 GPU 메모리가 허용하는 범위 내에서 데이터를 나누어 모델에 통과시키고 최종 송실 값에 더해주어 해결할수있다.\n",
    "    - 학습 속도의 문제가 발생할 수 있다. 한 번의 파라미터 업데이트를 위해 전체 데이터를 모델에 통과시키는 계산을 하는 것은 비효율적이다.\n",
    "        - 이런 경우에 확률적 경사하강법을 통해 문제를 해결할 수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stochastic Gradient Descent(SGD)는 전체 데이터셋을 모델에 통과시키는 대신 랜덤 샘플링한 k개의 샘플을 모델에 통과시켜 손실 값을 계산하고\n",
    "- 미분 이후에 파라미터 업데이트를 수행한다.\n",
    "- 이때 샘플링 과정에서 주의할 것은 비복원 추출을 수행한다는 점이다.\n",
    "- 즉, 한 번에 학습에 활용된 샘플은 모든 샘플들이 학습에 활용될 때까지 다시 학습에 활용되지 않는다.\n",
    "- 이처럼 샘플링을 하는 과정이 확률적으로 동작하므로 기존 경사하강법에 확률적이라는 단어가 붙어 확률적 경사하강법이라고 부른다.\n",
    "- 또한 랜덤 샘플링된 k개의 심플들의 묶음을 미니배치(mini-batch)라고 부른다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 즉, 샘플링 과정이 비복우너 추출이기 때문에 모델을 통과하지 않은 샘플이 더 이상 없을 때까지 기존에 모델을 통과한 샘플들은 사용되지 않는다.<br><Br>\n",
    "> 다시 말해 남아 있는 샘플들이 없을 때까지 비복우너 램덤 추출이 수행되며 샘플들이 전부 소진되면 다시 전체 샘플들에 대해 비복우너 추출이 진행된다.<br><br>\n",
    "> 이렇게 전체 데이터셋의 샘플들이 전부 모델을 통과하는 것을 한번의 에포크(epoch)라고 부르며, 한 개의 미니배치를 모델에 통과시키는 것을 이터레이션(iteration):파라미터업데이트이라고 부른다.<br><br>\n",
    "> 미니배치의 크기가 작아질수록 파라미터 업데이트 횟수가 늘어나는 것을 알 수 있다.<br><br>\n",
    "> 또한 SGD를 실제로 구현하기 위해서는 자연스럽게 이중 for-loop가 만들어질 수 밖에 없다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 미니 배치 크기는 학습률과 마찬가지로 사용자에 의해 정해지는 하이퍼파라미터가 된다.\n",
    "- 따라서 미니배치의 크기에 따라 학습의 성향이 바뀌기도 한다.\n",
    "- 보통 실무에서는 미니배치의 크기를 GPU 메모리 허용 범위 안에서 크게 잡는다.\n",
    "- 하지만 4000이 넘는 등의 너무 큰 미니배치 크기를 활용할 경우에는 오히려 성능 저하가 생길 수 있다.\n",
    "- 특별한 경우를 제외하고는 보통 256이나 512정도의 크기가 적당하다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c76801ef4636d259bef71178ff9b6783756e10e36adbdb1b3ea353d89da04bd9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

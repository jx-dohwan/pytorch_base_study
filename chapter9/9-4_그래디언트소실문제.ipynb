{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그래디언트 소실 문제\n",
    "- 심층 신경망이 너무 깊어지면 최적화가 잘 수행되지 않는 문제가 종종 발생한다.\n",
    "- 특히 입력에 가까운 계층의 가중치 파라미터가 잘 업데이트되지 않아 생기는데 이러한 것을 그래디언트 소실(gradient vanishing)문제라과 부른다.\n",
    "- 시그모이드는 전 구간에서 1보다 한참 작은 기울기를 가지며, tanh의 경우는 전 구간에서 1보다 같거나 작은 값을 갖는다.\n",
    "- 따라서 체인룰을 통해 펼쳐보면 계속해서 1보다 같거나 작은 값이 반복적으로 곱해진다.\n",
    "- 결과적으로 그래디언트는 더 작은 그래디언트를 갖게 되고 결국 0에 근접하게 되어 \n",
    "- 입력에 가까운 계층의 가중치 파라미터는 업데이트 양이 거의 없게 될 것이다.\n",
    "- 예를 들어 W1은 데이터에 대하여 알맞은 출력을 반환하기 위해 학습이 되지 않는 것이다. \n",
    "- 이처럼 깊어지는 심층신경망에서 입력에 가까운 게층이 잘 학습되지 않는 문제를 그래디언트 소실이라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c76801ef4636d259bef71178ff9b6783756e10e36adbdb1b3ea353d89da04bd9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

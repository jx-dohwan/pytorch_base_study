{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 미니배치 크기에 따른 SGD\n",
    "- 만약 미니배치의 크기가 매우 커서 점점 데이터셋의 크기에 가까워 진다면 어떻게 될까?\n",
    "- 아마도 SGD를 통해 얻은 그래디언트의 방향과 GD를 통해 얻은 그래디언트의 방향이 비슷해질 것이다.\n",
    "- 즉, 좀 더 정확한 그래디언트를 얻을 수 있다고 볼 수 있다. 하지만 샘플 수가 많아진 만큼 계산량도 늘어나게 된다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 반대로 미니배치의 크기가 작아져서 점점 1에 가까워진다면 \n",
    "- 랜덤 샘플링된 미니배치의 분포는 전체 데이터셋의 분포와 달라 편향을 가지게 될 가능성이 높다\n",
    "- 물론 이로 인해 전체 데이터셋의 손실표면에서 지역최소점은 탈출할 수 있다.\n",
    "- 하지만 결국 그래디언트에 너무 심한 노이즈가 생길 수 있으며 이로 인해 파라미터가 올바른 방향으로 학습되는 것을 저해할 수도 있다.\n",
    "- 따라서 적절한 미니배치 크기를 정해줘야 하는데 보통 256 정도의 크기에서 시작하는 것이 좋다\n",
    "    - 메모리가 모자라다면 더 줄이는 방향\n",
    "    - 메모리가 남고 성능의 하락이 없다면 더 늘리는 방향으로"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 미니배치 크기에 따른 파라미터 업데이트 횟수\n",
    "- 한 epoch에서 파라미터 업데이트 횟수는 이터레이션 횟수와 같다.\n",
    "- 즉, 이터레이션이 많을수록 신경망은 학습할 기회가 많아진다.\n",
    "- 그러면 미니배치 크기를 줄여서 이터레이션 숫자를 마냥 늘리면 될것 같지만 너무 줄이게 되면 노이즈가 발생할 수 있다.\n",
    "- 반대로 미니배치 크기를 늘려서 이터레이션 숫자를 줄이면 오히려 그래디언트의 방향은 좀 더 정확해질 수 있다.\n",
    "- 그렇다면 차라리 이터레이션 횟수를 줄이되 학습률을 크게 잡아 파라미터가 업데이트되는 양을 늘릴 수도 있을 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

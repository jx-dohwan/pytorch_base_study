{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 가중치 감쇠\n",
    "- 가장치 파라미터가 학습되는 것을 방해하여 오버피팅을 방지하는 정규화기법이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가중치 감쇠의 수식 표현\n",
    "- 가중치 감쇠 기법은 기본적으로 손실 함수를 수정하는 방법을 통해 적용된다.\n",
    "- 가중치 파라미터의 L2 norm을 구하고 기존의 손실 함수와 함께 이 norm을 같이 최소화하도록 함을써 모델의 학습을 방해하는 형태로 동작한다.\n",
    "- 가중치 파라미터의 L2 norm과 기존의 손실 값을 동시에 최소화하도록 손실 함수가 수정되었다.\n",
    "- 또한 α라는 하이퍼파라미터가 추가되어 L2 norm과 기존의 손실 값 사이의 균형을 맞추도록 한다.\n",
    "- 주의할 점은 가중치 파라미터 중에서 편향 b는 가중치 감쇠에서 제외된다는 것이다.\n",
    "- 이 손실 함수 때문에 가중치 파라미터가 학습 과정에서 점점 원점으로부터 멀어지는 것을 방지하는 효과가 발생합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 왜?\n",
    "- 가중치 파라미터에서 W의 요소들은 선형 계층에서 입출력 노드 사이의 관계를 나타냅니다.\n",
    "- 요소값의 크기가 클수록 강한 관계임을 의미한다고 볼 수 있다.\n",
    "- 이때 W에 L2 norm을 취하여 관계를 약화함으로써 노드 사이 관계의 강도를 제한하여 각 계층의 출력노드가 다수의 입력 노드로부터 많이 학습하는 것을 제한하게 됩니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 norm의 사용\n",
    "- L2 norm 대신에 L1 norm을 사용하여 가중치 감쇠를 실행할 수도 있다.\n",
    "- L1 norm도 마찬가지로 가중치 파라미터의 크기를 나타내지만 조금 특성이 다르다.\n",
    "- L1 norm을 활용하여 가중치 감쇠를 적용할 경우, 가중치 파라미터의 값들이 희소하도록 제한한다.\n",
    "- L1의 경우 점선으로 구성된 사각형 위의 점들이 원점으로부터 같은 거리로 인식하게 된다\n",
    "- 보통 벡터나 행렬이 희소할수록 많은 요소 값들이 0이 된다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가중치 감쇠의 구현\n",
    "- 가중치 감쇠의 경우 신경망을 구성할 때 계층을 추가하는 형태가 아니라 파이토치 옵티마이저를 생성할때 설정을 통해 구현\n",
    "- weight_decay 파라미터가 앞의 수식의 하이퍼파라미터 α를 의미한다.\n",
    "- 기본 설정 값은 0으로 되어 있으므로 가중치 감쇠가 ㅈ거용되지 않도록 설정되어 있음을 확인할 수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실 함수의 최소화를 방해하는 성질\n",
    "- 가중치 감쇠를 그다지 선호하지 않는다. 이유는 이후에 소개할 다른 정규화 기법에 비해 너무 학습을 방해하는 성격이 강해 좋은 성과를 얻지 못한 경험이 많다.\n",
    "- 이처럼 가중치 감쇠는 손실 함수에서 최소화를 방해하는 수식이 추가됨으로써 정규화를 달성하고자 한다.\n",
    "- 해당 파라미터에 따라 모델의 성능이 결정되게 된다.\n",
    "> 이처럼 가중치 감쇠와 같은 다양한 정규화 기법들은 보통 학습을 방해하는 형태를 취함으로 오버피팅을 최소화하는 목적을 달성하고자 한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비록 가중치 감쇠가 널리 쓰이는 정규화 방법이 아닐 수도 있지만 손실 함수 수정을 통한 정규화 방법의 대표주자라고 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

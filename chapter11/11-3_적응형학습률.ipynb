{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 적응형 학습률\n",
    "- 기존 경사하강법에 여러가지 기법ㅇ르 더함으로써 좀 더 수월하게 가중치 파라미터를 최적화 할 수 있다.\n",
    "- 학습률은 대표적인 하이퍼파라미터이다. 값이 너무 크면 학습이 안정적으로 이루어지지 않고 너무 작으면 학습 시간이 더뎌지기 때문에 가장 먼저 튜닝이 필요한 파라미터이기도 하다"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습률의 크기에 따른 특성과 동적 학습률의 필요성\n",
    "- 갈 길이 먼 학습 초반에는 큰 학습률을 가져가고 학습이 진행됨에 따라 점점 작은 학습률을 가져가는 방법도 있다.\n",
    "- 아예 필요한 학습률을 자동으로 찾아서 적용해주는 방법이 있다.\n",
    "- 요즘 가장 많이 쓰이는 적응형 학습률에 대한 알고리즘은 아담(Adam)이다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모멘텀(momentum)\n",
    "- 모멘컴은 시작부터 매번 계산된 그래디언트를 누적하는 형태로 구현된다.\n",
    "- 이를 통해서 지역최소점을 쉽게 탈출 할 수 있고 학습 속도를 가속화할 수 있다.\n",
    "- 따라서 관성이라는 의미의 단어인 모멘텀이라는 이름이 붙여졌다.\n",
    "- 오른쪽으로 가야하는 간단한 문제인데 위아래 왔다갔다 나느라 최적화가 더디게 진행되어 비효율적인 상황이 될 수 있다.\n",
    "- 이때 모멘텀을 적용한다면 위아래로 진동하는 방향이 서로 상쇄되어 작아질 것이고 오른쪽으노 나아가는 방향은 계속 누적되어 커질 것이다.\n",
    "- 즉, 위아래 진동은 줄어들고 오른쪽으로 진행되는 속도는 빨라져 우리가 원하는 형태로 최적화가 더 빨리 진행될 수 있을 것이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 적응형 학습률\n",
    "- 모멘텀은 여전히 학습률의 튜닝이 필요하다는 아쉬움이 있다. 즉, 학습률의 크기에 따라 최적화 여부가 좌지우지될 수 있다.\n",
    "- 따라서 학습률 자체를 따로 튜닝하지 않아 대부분의 상황에서 최적화가 수행될 수 있는 방법을 연구가 많이 진행되고 있다.\n",
    "- 결국 상황에 따라 학습률이 자동으로 정해지는 형태가 ㅗ딜 것이고 이것을 적응형 학습률(adaptive learning rate)이라고 부른다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습률 스케줄링\n",
    "- 학습 초반은 어차피 갈 길이 멀기 때문에 학습률을 크게 가져가면 좋고\n",
    "- 학습 후반에 갈수록 미세한 가중치 파라미터 조정이 필요할 수 있기 때문에 학습률이 작아지면 좋을 것이라고 생각 할 수 있다.\n",
    "- 그러나 작은 학습률로 시작하면 초반에 더진 진행을, 큰 학습률로 시작하면 후반에 미세한 파라미터 조정이 어려워질 것이다.\n",
    "- 따라서 이런 이유로 학습률 스케줄링 기법을 활용하기도 한다.\n",
    "    - 초기 학습률을 가지고 모델 최적화를 수행한다.\n",
    "    - 일정 에포크 또는 이터레이션이 경과한 후에 감쇠(decay)를 시작한다. 또는 해당 에포크에서 모델의 학습이 더 이상 진전되지 않을 때 감쇠를 하기도 한다.\n",
    "    - 학습률 감쇠는 선형적으로 적용될 수 있으며 비율(ratio)이 곱해지는 형태로 적용되거나 코사인(cosine)함수의 형태로 적용될 수 있다.\n",
    "\n",
    "> 학습률과 관련한 하이퍼파라미터가 더 추가되는 아쉬움이 있어 학습률 스케줄에 모델의 성능이 굉장히 민감한 경우엔느 스케줄 튜닝에 있어 어려움을 겪기도 한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아다그래드 옵티마이저\n",
    "- 아다그래드(AdaGrad)는 최초로 제안된 적응형 학습률 알고리즘이라고 할 수 있다.\n",
    "- 아다그래드의 가장 큰 특징은 가중치 파라미터마다 별도의 학습률을 가진다는 것이다.\n",
    "- 각 가중치 파라미터의 학습률은 가중치 파라미터가 업데이트될수록 반비례하여 작아지게 된다.\n",
    "- 문제는 학습이 진행됨에 따라 파라미터 업데이트가 많이 될 경우 학습률이 너무 작아져 \n",
    "- 나중에는 그래디언트가 크더라도 가중치 파라미터 업데이트가 잘 이루어지지 않을 수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아담 옵티마이저\n",
    "- 아담 옵티마이저(Adam optimizer)는 기존 적응형 학습률 방식에 모멘텀이 추가된 알고리즘으로 기존 알고리즘들을 보완함으로써 가장 보편적으로 쓰이는 알고리즘이다.\n",
    "- 기본 설정값을 가지고도 대부분의 문제에 좋은 성능을 낼 수 있다.\n",
    "- 즉, 모델이나 학습 방법이 학습률에 강인해지는 효과를 볼 수 있다.\n",
    "- 일단은 아담 기본 세팅으로 사용해도 좋다 나중에는 이것도 튜닝해야하는 경우도 있긴하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c76801ef4636d259bef71178ff9b6783756e10e36adbdb1b3ea353d89da04bd9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

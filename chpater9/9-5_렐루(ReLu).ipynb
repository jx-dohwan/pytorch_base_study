{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 렐루(ReLU)\n",
    "- 앞서 그래디언트 소실 문제에 대해서 이야기 했었다.\n",
    "- 이것은 활성 함수의 미분 계수가 전 구간에서 1보다 작기 때문에 발생하는 것이다.\n",
    "- 또한 시그모이드나 tanh의 경우 함수의 양 끝단에 갈수록 기울기가 0이 되는 형태를 띠고 있다.\n",
    "- 그렇기 때문에 모델의 학습이 진행되어 선형 계층의 결괏값이 0으로부터 멀어져 어떤 의미를 지니게 될수록 학습이 점점 더져진다는 단점이 있다.\n",
    "- 즉 같은 성능에 도달하기 위해서 수행되어야 하는 경사하강법을 통한 업데이트 횟수가 많을수밖에 없다"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/relu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c76801ef4636d259bef71178ff9b6783756e10e36adbdb1b3ea353d89da04bd9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

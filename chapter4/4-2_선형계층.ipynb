{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선형계층\n",
    "- 행렬 곱셈과 벡터의 덧셈으로 이루어져 있기 때문에 선형 변환이라고 볼 수 있다.\n",
    "- 따라서 선형 계층을 통해 모델을 구성할 경우에는 선형 데이터에 대한 관계를 분석하거나 선형 함수를 근사계산할수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.FloatTensor([[1,2],[3,4],[5,6]])\n",
    "b = torch.FloatTensor([2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x,W,b):\n",
    "    y = torch.matmul(x, W) + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "y = linear(x,W,b)\n",
    "print(y.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn.Module 클래스상속받기\n",
    "- 파이토치에는 nn(neural networks)패키지가 있고 내부에는 미리 정의된 많은 신경망들이 있다.\n",
    "- 그리고 그 신경망들은 torch.nn.Module이라는 추상 클래스를 상속받아 정의되어있다.\n",
    "- 바로 이 추상 클래스를 상속받아 선형 계층을 구현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> nn.Module를 상속받으면 보통 2개의 메서드 __init__과 forward를 오버라이드한다.<br>\n",
    "> __init__ 함수는 계층 내부에서 필요한 변수를 미리 선언하고 있으며 심지어 또 다른 계층을 소유하도록 할 수 있다.<br>\n",
    "> forward함수는 계층을 통과하는데 필요한 계산 수행을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=3, output_dim=2):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.W = torch.FloatTensor(input_dim, output_dim)\n",
    "        self.b = torch.FloatTensor(output_dim)\n",
    "\n",
    "    # You should overrid 'forward' method to implement detail\n",
    "    # The input argments and outputs can be designed as you wish.\n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, input_dim)\n",
    "        y = torch.matmul(x, self.W) + self.b\n",
    "        # |y| = (batch.size, input_dim) * (input_dim, output_dim)\n",
    "        # = (batch_size, output_dim)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = MyLinear(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward 함수를 따로 호출하지 않고 객체명에 바로 괄호를 열어 텐서 x를 인수로 넘겨주었다.\n",
    "# 이처럼 nn.Module의 상속받은 객체는 __call__함수와 forward가 매핑되어 있어서 forward를 직접 부를 필요가 없다.\n",
    "# forward 호출 앞뒤로 추가적으로 호출하는 함수가 파이토치 내부에 따로 있기 때문에 사용자가 직접 forward 함수를 호출하는 것을 권장하지 않는다.\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 올바른 방법 : nn.Parameter 활용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=3, output_dim=2):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim))\n",
    "        self.b = nn.Parameter(torch.FloatTensor(output_dim))\n",
    "\n",
    "    # You should overrid 'forward' method to implement detail\n",
    "    # The input argments and outputs can be designed as you wish.\n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, input_dim)\n",
    "        y = torch.matmul(x, self.W) + self.b\n",
    "        # |y| = (batch.size, input_dim) * (input_dim, output_dim)\n",
    "        # = (batch_size, output_dim)\n",
    "\n",
    "        return y\n",
    "linear = MyLinear(3,2)\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Linear 활용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=3, output_dim=2):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    # You should overrid 'forward' method to implement detail\n",
    "    # The input argments and outputs can be designed as you wish.\n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, input_dim)\n",
    "        y = self.linear(x)\n",
    "        # |y| = (batch.size, input_dim) * (input_dim, output_dim)\n",
    "        # = (batch_size, output_dim)\n",
    "\n",
    "        return y\n",
    "linear = MyLinear(3,2)\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3912, -0.3466],\n",
      "        [-0.3912, -0.3466],\n",
      "        [-0.3912, -0.3466],\n",
      "        [-0.3912, -0.3466]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c76801ef4636d259bef71178ff9b6783756e10e36adbdb1b3ea353d89da04bd9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
